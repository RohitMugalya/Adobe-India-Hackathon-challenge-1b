# Challenge 1b: Multi-Collection PDF Analysis

## Overview
Advanced PDF analysis solution using transformer models and semantic similarity to extract relevant content from document collections based on specific personas and tasks. The system operates completely offline using locally cached models.

## Project Structure
```
pdf-analyzer/
├── models/                          # Cached transformer models
│   ├── models--facebook--bart-base/
│   └── sentence_transformers/
├── venv/                           # Python virtual environment (optional)
├── pdf_analyzer_transformers.py    # Main analysis tool
├── model_init.py                   # Model download script
├── docker_run.py                   # Docker execution script
├── approach_explanation.md         # Methodology documentation
├── requirements.txt                # Python dependencies
├── Dockerfile                      # Docker container configuration
├── .dockerignore                   # Docker build optimization
├── build_docker.sh                # Docker build script
├── test_docker.py                 # Docker testing utility
├── run_analysis.py                # Legacy analysis script
└── README.md                       # This file

# Input structure (for Docker execution)
input/
├── Collection1/                    # Example: Travel Planning
│   ├── challenge1b_input.json     # Input configuration
│   └── PDFs/                      # PDF documents
│       ├── document1.pdf
│       └── document2.pdf
├── Collection2/                    # Example: Adobe Acrobat Learning
│   ├── challenge1b_input.json
│   └── PDFs/
│       └── tutorial.pdf
└── Collection3/                    # Example: Recipe Collection
    ├── challenge1b_input.json
    └── PDFs/
        └── cookbook.pdf

# Output structure (generated by Docker)
output/
├── Collection1_output.json        # Analysis results
├── Collection2_output.json
└── Collection3_output.json
```

## Example Use Cases

The system is designed to work with diverse document collections and personas. Here are some example scenarios:

### Example 1: Travel Planning
- **Challenge ID**: round_1b_002
- **Persona**: Travel Planner
- **Task**: Plan a 4-day trip for 10 college friends to South of France
- **Documents**: 7 travel guides (Cities, Cuisine, History, Hotels, Activities, Tips, Culture)

### Example 2: Adobe Acrobat Learning
- **Challenge ID**: round_1b_003
- **Persona**: HR Professional
- **Task**: Create and manage fillable forms for onboarding and compliance
- **Documents**: 15 Acrobat tutorial guides

### Example 3: Recipe Collection
- **Challenge ID**: round_1b_001
- **Persona**: Food Contractor
- **Task**: Prepare vegetarian buffet-style dinner menu for corporate gathering
- **Documents**: 9 cooking guides and recipe collections

### Supported Document Types
- Research papers and academic publications
- Technical manuals and guides
- Business reports and financial documents
- Educational textbooks and materials
- News articles and journalistic content
- Any PDF document with structured text content

## Technical Approach

### Core Technologies
- **BART Model**: `facebook/bart-base` for text generation and content extraction
- **Sentence Transformers**: `all-mpnet-base-v2` for semantic similarity analysis
- **Cross-Encoder**: `ms-marco-MiniLM-L-6-v2` for document re-ranking (optional)
- **Cosine Similarity**: For document relevance detection and importance ranking

### Key Features
- **Semantic Document Filtering**: Uses cosine similarity to identify relevant documents
- **Hybrid Ranking System**: Combines bi-encoder and cross-encoder models for accurate importance ranking
- **CPU-Optimized**: Automatically uses all available CPU cores for parallel processing
- **Offline Operation**: All models cached locally, no internet required after setup
- **Generalized Approach**: Works with any persona/task combination without hardcoding

## Installation and Setup

### Prerequisites
- **Python 3.11.4** (recommended for optimal performance)
- **Docker** (for containerized deployment)
- **Internet connection** (for initial model download only)

### Step 1: Install Dependencies
```bash
pip install -r requirements.txt
```

### Step 2: Download Models (One-time setup)
```bash
# Download and cache all required models
python model_init.py

# Check if models are already downloaded
python model_init.py --check

# Force re-download models
python model_init.py --force
```

This will download and cache:
- `facebook/bart-base` (~500MB)
- `all-mpnet-base-v2` (~420MB)
- `ms-marco-MiniLM-L-6-v2` (~90MB, optional)

### Step 3: Run Analysis (Local Development)
```bash
# Process any collection directory (automatically uses all CPU cores)
python pdf_analyzer_transformers.py "/path/to/your/collection"

# Example with local collection directories:
python pdf_analyzer_transformers.py "input/Collection1"
python pdf_analyzer_transformers.py "input/Collection2"
```

### Step 4: Docker Deployment (Recommended for Production)
```bash
# Build Docker image
docker build --platform linux/amd64 -t pdf-analyzer:round1b .

# Run with your input/output directories
docker run --rm \
  -v $(pwd)/input:/app/input \
  -v $(pwd)/output:/app/output \
  --network none \
  pdf-analyzer:round1b
```

## Data Format

### Input JSON Structure
Each collection requires a `challenge1b_input.json` file:
```json
{
  "challenge_info": {
    "challenge_id": "round_1b_002",
    "test_case_name": "travel_planning"
  },
  "documents": [
    {"filename": "document1.pdf", "title": "Travel Guide"},
    {"filename": "document2.pdf", "title": "City Information"}
  ],
  "persona": {"role": "Travel Planner"},
  "job_to_be_done": {"task": "Plan a 4-day trip for 10 college friends"}
}
```

### Output JSON Structure
The system generates `{CollectionName}_output.json` files:
```json
{
  "metadata": {
    "input_documents": ["document1.pdf", "document2.pdf"],
    "persona": "Travel Planner",
    "job_to_be_done": "Plan a 4-day trip for 10 college friends",
    "processing_timestamp": "2025-07-28T10:30:45.123456",
    "model_used": "facebook/bart-base",
    "similarity_model": "all-mpnet-base-v2 + cross-encoder"
  },
  "extracted_sections": [
    {
      "document": "document1.pdf",
      "section_title": "Travel Destinations",
      "importance_rank": 1,
      "page_number": 2
    }
  ],
  "subsection_analysis": [
    {
      "document": "document1.pdf",
      "refined_text": "Detailed analysis of travel destinations...",
      "page_number": 2
    }
  ]
}
```

## How It Works

### 1. Document Relevance Detection
- Converts document filenames to semantic embeddings
- Compares against task description using cosine similarity
- Filters out documents with similarity < 0.3 threshold

### 2. Importance Ranking
- Ranks relevant documents by semantic similarity to the task
- Applies cross-encoder re-ranking for improved accuracy
- Assigns unique ranks from 1 (most important) to N (least important)

### 3. Content Extraction
- Extracts section titles using regex patterns and BART generation
- Analyzes first 5 pages of each relevant document
- Generates coherent paragraph summaries for subsection analysis

### 4. Parallel Processing
- Automatically detects and uses all available CPU cores
- Thread-safe processing with concurrent document analysis
- Optimized for CPU-only environments

## System Capabilities

### Performance Characteristics
- **CPU Optimized**: Uses all available cores automatically
- **Memory Efficient**: Loads models once, processes multiple documents
- **Fast Processing**: Parallel document analysis with semantic filtering
- **Offline Operation**: No internet required after initial setup
- **Scalable**: Handles collections of any size

### Output Quality
- **extracted_sections**: One entry per relevant document (no duplicates)
- **subsection_analysis**: Detailed content analysis for relevant documents only
- **metadata**: Processing information and model details

## Docker Deployment (Adobe India Hackathon 2025 - Round 1B)

### Quick Start with Docker

#### 1. Build the Docker Image
```bash
# Build the Docker image for AMD64 architecture
docker build --platform linux/amd64 -t pdf-analyzer:round1b .
```

#### 2. Prepare Input Directory
Create an input directory with your collections:
```bash
mkdir -p input/Collection1/PDFs
mkdir -p input/Collection2/PDFs
mkdir -p output
```

#### 3. Run the Container
```bash
# Run the container with input/output volume mounts and no network access
docker run --rm \
  -v $(pwd)/input:/app/input \
  -v $(pwd)/output:/app/output \
  --network none \
  pdf-analyzer:round1b
```

### Docker Input Structure
```
input/
├── Collection1/
│   ├── challenge1b_input.json      # Required: Contains persona and task
│   └── PDFs/                       # Required: Contains PDF documents
│       ├── document1.pdf
│       ├── document2.pdf
│       └── document3.pdf
├── Collection2/
│   ├── challenge1b_input.json
│   └── PDFs/
│       ├── guide1.pdf
│       └── guide2.pdf
```

### Docker Output Structure
```
output/
├── Collection1_output.json         # Results for Collection1
├── Collection2_output.json         # Results for Collection2
```

### Hackathon Compliance (Round 1B Requirements)

#### ✅ Technical Constraints
- **Architecture**: AMD64 (x86_64) compatible
- **CPU Only**: No GPU dependencies, optimized for CPU processing
- **Model Size**: <1GB total (BART-base ~500MB + all-mpnet-base-v2 ~420MB)
- **Offline Operation**: No network access required after build
- **Processing Time**: <60 seconds per collection (3-5 documents)
- **Memory Usage**: Optimized for 16GB RAM systems
- **CPU Cores**: Automatically uses all 8 available CPU cores

#### ✅ Docker Requirements
- **Platform**: `--platform linux/amd64` compatible
- **Network**: Runs with `--network none` (offline)
- **Volumes**: Input/output directory mounting
- **Base Image**: python:3.11.4-slim for optimal performance

#### ✅ Execution Compliance
```bash
# Expected build command
docker build --platform linux/amd64 -t mysolutionname:somerandomidentifier .

# Expected run command
docker run --rm \
  -v $(pwd)/input:/app/input \
  -v $(pwd)/output:/app/output \
  --network none \
  mysolutionname:somerandomidentifier
```

### Container Specifications
- **Base Image**: python:3.11.4-slim (AMD64)
- **Python Version**: 3.11.4 (latest stable)
- **Models**: Pre-downloaded during build (offline ready)
- **Dependencies**: All Python packages included in container
- **Working Directory**: /app
- **Input Mount Point**: /app/input
- **Output Mount Point**: /app/output
- **Execution Script**: docker_run.py (automatic processing)

### Performance Optimization
- **Parallel Processing**: Utilizes all available CPU cores
- **Memory Efficient**: Loads models once, processes multiple collections
- **Fast Startup**: Models pre-cached during build
- **Optimized Libraries**: Uses CPU-optimized versions of PyTorch and Transformers

### Testing Your Setup
Use the provided test script to verify your Docker setup:
```bash
# Create test environment
python test_docker.py

# Build and test
./build_docker.sh

# Run test
docker run --rm \
  -v $(pwd)/test_input:/app/input \
  -v $(pwd)/test_output:/app/output \
  --network none \
  pdf-analyzer:round1b
```

### Troubleshooting

#### Common Issues
- **Build Issues**: Ensure Docker supports AMD64 platform and sufficient disk space (>2GB)
- **Runtime Issues**: Verify input directory structure and PDF file readability
- **Performance Issues**: Processing time scales with document count and complexity
- **Memory Issues**: Ensure minimum 4GB RAM, close other applications during processing
- **Model Issues**: Re-download models with `python model_init.py --force`

---

**Note**: This solution uses transformer models with semantic similarity analysis for accurate, persona-specific PDF analysis. Built for Adobe India Hackathon 2025 - Round 1B. 